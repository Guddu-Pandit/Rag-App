# ðŸ¤– RAG Assistant

A modern **Retrieval-Augmented Generation (RAG)** chatbot built with Next.js 16, LangGraph, Pinecone, and Google Gemini. Upload documents to build your knowledge base and chat with AI that answers based on your content.

<div>

![Modern Dashboard](https://img.shields.io/badge/UI-Premium-success)
![AI Powered](https://img.shields.io/badge/AI-Google_Gemini-blue)
![Database](https://img.shields.io/badge/DB-Supabase_%26_Pinecone-green)

</div>

## âœ¨ Features

- **ðŸ“„ Document Upload** - Support for PDF, TXT, and DOCX files
- **ðŸ§  Smart RAG Pipeline** - LangGraph-powered workflow with vector similarity search
- **ðŸ’¬ Multi-Conversation Support** - Create, switch, and manage multiple chat sessions
- **ðŸŽ¨ Premium Dark UI** - Sleek glassmorphism design with teal accents
- **âš¡ Real-time Streaming** - Smooth typing animation for responses
- **ðŸ’¾ Persistent Storage** - Conversations saved to localStorage

## ðŸ› ï¸ Tech Stack

| Category | Technology |
|----------|------------|
| **Framework** | Next.js 16 (App Router) |
| **AI/LLM** | Google Gemini 2.5 Flash Lite |
| **Orchestration** | LangGraph |
| **Vector DB** | Pinecone |
| **Storage** | Supabase |
| **Styling** | Tailwind CSS 4 |
| **Components** | Radix UI + Lucide Icons |

## ðŸš€ Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/Guddu-Pandit/Rag-App.git
cd Rag-App
npm install
```

### 2. Configure Environment

Create a `.env` file in the root directory:

```env
# Supabase
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key
BUCKET_NAME=rag_app

# Pinecone
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX=rag

# Google Gemini
GEMINI_APT_KEY=your_gemini_api_key

# LangSmith (Optional- For Tracing)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key
LANGCHAIN_PROJECT=Rag-App
```

### Environment Variables Reference

| Variable | Description |
|----------|-------------|
| `NEXT_PUBLIC_SUPABASE_URL` | Your Supabase project URL |
| `NEXT_PUBLIC_SUPABASE_ANON_KEY` | Supabase anonymous/public key |
| `SUPABASE_SERVICE_ROLE_KEY` | Supabase service role key (for server-side operations) |
| `BUCKET_NAME` | Supabase storage bucket name for documents |
| `PINECONE_API_KEY` | Your Pinecone API key |
| `PINECONE_INDEX` | Pinecone index name for vector storage |
| `GEMINI_API_KEY` | Google Gemini API key for LLM and embeddings |
| `LANGCHAIN_TRACING_V2` | Set to `true` to enable LangSmith tracing (optional) |
| `LANGCHAIN_API_KEY` | LangSmith API key for tracing (optional) |
| `LANGCHAIN_PROJECT` | LangSmith project name (optional) |

### 3. Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to start chatting!

## ðŸ“ Project Structure

```
ragapp/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/          # Chat endpoint
â”‚   â”‚   â”œâ”€â”€ documents/     # Document CRUD
â”‚   â”‚   â””â”€â”€ upload/        # File upload handler
â”‚   â””â”€â”€ page.tsx           # Main chat interface
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ChatInput.tsx      # Message input
â”‚   â”œâ”€â”€ ChatMessage.tsx    # Message display
â”‚   â”œâ”€â”€ ConversationSidebar.tsx
â”‚   â””â”€â”€ DocumentUpload.tsx
â””â”€â”€ lib/
    â”œâ”€â”€ embeddings.ts      # Gemini embeddings
    â”œâ”€â”€ langgraph.ts       # RAG workflow (Hierarchical Tracing)
    â”œâ”€â”€ pinecone.ts        # Vector DB client
    â””â”€â”€ supabase.ts        # Storage client
```

## ðŸ”§ How It Works


```mermaid
graph TD
    A[ðŸ“„ User Upload] -->|Chunk & Store| B[ðŸ—„ï¸ Supabase Storage]
    A -->|Embed| C[âš¡ Gemini Embeddings]
    C -->|Vector Index| D[ðŸŒ² Pinecone DB]
    E[ðŸ‘¤ User Query] -->|Search| D
    D -->|Retrieve Context| F[ðŸ§  LangGraph Workflow]
    F -->|Generate Answer| G[ðŸ¤– Gemini AI]
```

1. **ðŸ“¥ Ingestion** â†’ Upload documents (PDF, Docx) which are securely stored in Supabase.
2. **ðŸ§© Processing** â†’ Text is chunked and converted into vector embeddings.
3. **ðŸ’¾ Indexing** â†’ High-dimensional vectors are stored in Pinecone for semantic search.
4. **ðŸ” Retrieval** â†’ LangGraph orchestrates the search for relevant context.
5. **âœ¨ Generation** â†’ Gemini synthesizes the ANSWER using your specific data.


## ðŸ§  LangGraph Workflow

The intelligent orchestration layer manages the conversation flow with hierarchical tracing for **LangSmith**:

```mermaid
graph TD
    Start([Start]) --> Agent[ðŸ¤– Agent Node]
    subgraph Agent_Internal [Agent:Generate]
        Agent
    end
    
    Agent -->|CallTools| Tool[ðŸ” Tool Node]
    subgraph Tool_Internal [Tool:VectorSearch]
        Tool
    end
    
    Tool --> Agent
    Agent -->|Finish| End([End])
```

1. **Agent Node (`Agent:Generate`)**: The starting point of the graph. It uses Gemini 2.5 Flash Lite to decide whether to use the vector search tool or provide a final answer.
2. **Conditional Edge**: Routes to `Tool` if the LLM initiates a `PineconeVectorSearch` call, or to `END` if a final response is ready.
3. **Tool Node (`Tool:VectorSearch`)**: Executes the specialized `PineconeVectorSearch` tool to fetch context from the knowledge base.
4. **Loop Management**: After tool execution, the graph loops back to the `Agent` node to incorporate the retrieved context into the final answer.

## ðŸ“„ License
This project is licensed under the ISC License.

