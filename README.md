# ü§ñ RAG Assistant

A modern **Retrieval-Augmented Generation (RAG)** chatbot built with Next.js 16, LangGraph, Pinecone, and Google Gemini. Upload documents to build your knowledge base and chat with AI that answers based on your content.

<div>

![Modern Dashboard](https://img.shields.io/badge/UI-Premium-success)
![AI Powered](https://img.shields.io/badge/AI-Google_Gemini-blue)
![Database](https://img.shields.io/badge/DB-Supabase_%26_Pinecone-green)

</div>

## ‚ú® Features

- **üìÑ Document Upload** - Support for PDF, TXT, and DOCX files
- **üß† Smart RAG Pipeline** - LangGraph-powered workflow with vector similarity search
- **üí¨ Multi-Conversation Support** - Create, switch, and manage multiple chat sessions
- **üé® Premium Dark UI** - Sleek glassmorphism design with teal accents
- **‚ö° Real-time Streaming** - Smooth typing animation for responses
- **üíæ Persistent Storage** - Conversations saved to localStorage

## üõ†Ô∏è Tech Stack

| Category | Technology |
|----------|------------|
| **Framework** | Next.js 16 (App Router) |
| **AI/LLM** | Google Gemini 2.5 Flash |
| **Orchestration** | LangGraph |
| **Vector DB** | Pinecone |
| **Storage** | Supabase |
| **Styling** | Tailwind CSS 4 |
| **Components** | Radix UI + Lucide Icons |

## üöÄ Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/Guddu-Pandit/Rag-App.git
cd Rag-App
npm install
```

### 2. Configure Environment

Create a `.env` file in the root directory:

```env
# Supabase
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key
BUCKET_NAME=rag_app

# Pinecone
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX=rag

# Google Gemini
GEMINI_APT_KEY=your_gemini_api_key
```

### Environment Variables Reference

| Variable | Description |
|----------|-------------|
| `NEXT_PUBLIC_SUPABASE_URL` | Your Supabase project URL |
| `NEXT_PUBLIC_SUPABASE_ANON_KEY` | Supabase anonymous/public key |
| `SUPABASE_SERVICE_ROLE_KEY` | Supabase service role key (for server-side operations) |
| `BUCKET_NAME` | Supabase storage bucket name for documents |
| `PINECONE_API_KEY` | Your Pinecone API key |
| `PINECONE_INDEX` | Pinecone index name for vector storage |
| `GEMINI_APT_KEY` | Google Gemini API key for LLM and embeddings |

### 3. Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to start chatting!

## üìÅ Project Structure

```
ragapp/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/          # Chat endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documents/     # Document CRUD
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ upload/        # File upload handler
‚îÇ   ‚îî‚îÄ‚îÄ page.tsx           # Main chat interface
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ ChatInput.tsx      # Message input
‚îÇ   ‚îú‚îÄ‚îÄ ChatMessage.tsx    # Message display
‚îÇ   ‚îú‚îÄ‚îÄ ConversationSidebar.tsx
‚îÇ   ‚îî‚îÄ‚îÄ DocumentUpload.tsx
‚îî‚îÄ‚îÄ lib/
    ‚îú‚îÄ‚îÄ embeddings.ts      # Gemini embeddings
    ‚îú‚îÄ‚îÄ langgraph.ts       # RAG workflow
    ‚îú‚îÄ‚îÄ pinecone.ts        # Vector DB client
    ‚îî‚îÄ‚îÄ supabase.ts        # Storage client
```

## üîß How It Works


```mermaid
graph TD
    A[üìÑ User Upload] -->|Chunk & Store| B[üóÑÔ∏è Supabase Storage]
    A -->|Embed| C[‚ö° Gemini Embeddings]
    C -->|Vector Index| D[üå≤ Pinecone DB]
    E[üë§ User Query] -->|Search| D
    D -->|Retrieve Context| F[üß† LangGraph Workflow]
    F -->|Generate Answer| G[ü§ñ Gemini AI]
```

1. **üì• Ingestion** ‚Üí Upload documents (PDF, Docx) which are securely stored in Supabase.
2. **üß© Processing** ‚Üí Text is chunked and converted into vector embeddings.
3. **üíæ Indexing** ‚Üí High-dimensional vectors are stored in Pinecone for semantic search.
4. **üîç Retrieval** ‚Üí LangGraph orchestrates the search for relevant context.
5. **‚ú® Generation** ‚Üí Gemini synthesizes the ANSWER using your specific data.


## üß† LangGraph Workflow

The intelligent orchestration layer manages the conversation flow:

```mermaid
graph TD
    Start([Start]) --> Decide{Decide Action}
    Decide -->|"Need Context"| Retrieve[üîç Retrieve Nodes]
    Decide -->|"General Chat"| Generate[‚ú® Generate Answer]
    
    Retrieve --> Generate
    Generate --> Validate{Validate Answer}
    
    Validate -->|"Valid"| End([End])
    Validate -->|"Invalid (Max 3)"| End
    Validate -->|"Invalid"| Decide
```

1. **Decide Node**: Analyzes the query to determine if retrieval is needed (e.g., specific facts) or if it can be answered directly (e.g., greetings).
2. **Retrieve Node**: Fetches specialized context from Pinecone if required.
3. **Generate Node**: Produces an answer using Gemini 2.5 Flash.
4. **Validate Node**: Self-reflects on the answer to ensure it is factually correct and grounded in the context. If not, it loops back to retry (up to 3 times).

## üìÑ License
This project is licensed under the ISC License.

---

Developed with ‚ù§Ô∏è by [Guddu-Pandit](https://github.com/Guddu-Pandit)
